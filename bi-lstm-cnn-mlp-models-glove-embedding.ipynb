{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hate Speech Detection on the Davidson Dataset \n## Models : BI-LSTM +  CNN + MLP models -  GLOVE Embedding","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau,CSVLogger\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,GlobalMaxPool1D,Embedding,Conv1D, GlobalMaxPooling1D, Bidirectional,LSTM\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2022-08-14T10:58:53.976948Z","iopub.execute_input":"2022-08-14T10:58:53.977893Z","iopub.status.idle":"2022-08-14T10:59:00.234751Z","shell.execute_reply.started":"2022-08-14T10:58:53.977372Z","shell.execute_reply":"2022-08-14T10:59:00.233735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Extraction","metadata":{}},{"cell_type":"code","source":"nRowsRead = None # specify 'None' to read complete file\ndf0 = pd.read_csv('../input/hateoffensive/labeled_data.csv', delimiter=',', nrows = nRowsRead)\nnRow, nCol = df0.shape\n\n#Doing Transformation\nc=df0['class']\ndf0.rename(columns={'tweet' : 'text',\n                   'class' : 'category'}, \n                    inplace=True)\na=df0['text']\nb=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\ndf= pd.concat([a,b,c], axis=1)\ndf.rename(columns={'class' : 'label'}, \n                    inplace=True)\n\nhate, ofensive, neither = np.bincount(df['label'])\ntotal = hate + ofensive + neither\nprint('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n    total, hate, 100 * hate / total))\nprint('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n    total, ofensive, 100 * ofensive / total))\nprint('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n    total, neither, 100 * neither / total))\n    \nx= df['text']\ny=df['label']\n\ntexts = x\ntarget = y\n#tokensing the data\n\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(texts)\n#defining vocabulary length\nvocab_length = len(word_tokenizer.word_index) + 1\n\ndef embed(text_data): \n    return word_tokenizer.texts_to_sequences(text_data)\n\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\n#padding_data\ntrain_padded_sentences = pad_sequences(\n    embed(texts), \n    length_long_sentence, \n    padding='post'\n)\n\nembeddings_dictionary = dict()\nembedding_dim = 100\n\n# Loading GloVe-100D embedding_file\nwith open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary[word] = vector_dimensions\n\n# Loading embedding_vectors of words which comes in Glove files other will be equated to 0\n#defining embedding matrix shape\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n#creating embedding matrix\nfor word, index in word_tokenizer.word_index.items(): \n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \n#splitting dataset\nX_train, X_test, y_train, y_test = train_test_split(\n    train_padded_sentences, \n    target, \n    test_size=0.25\n)\n\nX_train, x_val, y_train, y_val = train_test_split(\n    X_train, \n    y_train,\n    test_size=0.1 )","metadata":{"execution":{"iopub.status.busy":"2022-08-14T10:59:00.237059Z","iopub.execute_input":"2022-08-14T10:59:00.237735Z","iopub.status.idle":"2022-08-14T10:59:20.776925Z","shell.execute_reply.started":"2022-08-14T10:59:00.237679Z","shell.execute_reply":"2022-08-14T10:59:20.775778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BI-LSTM","metadata":{}},{"cell_type":"code","source":"#defining glove bilstm model\ndef bilstm():\n    model = Sequential()\n    #adding embediing layer\n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence))\n    #adding Bi_lstm later\n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2)))\n    model.add(GlobalMaxPool1D()) #globalmaxpooling_layer\n    model.add(BatchNormalization()) #bath_normalisation\n    model.add(Dropout(0.5)) #dropout_1\n    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_1\n    model.add(Dropout(0.5)) #dropout_2\n    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_2\n    model.add(Dropout(0.5)) #dropout_3\n    model.add(Dense(3, activation = 'softmax')) #classification_layer\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model\n\nbilstm_model = bilstm()\n\n#defining_class_weight for each class\nweight_class1 = (1 / hate)*(total)/3.0 \nweight_class2 = (1 / ofensive)*(total)/3.0\nweight_class3 = (1 / neither)*(total)/3.0\nclass_weight = {0: weight_class1, 1: weight_class2, 2: weight_class3}\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\n\nepoch_count=10\nbatch_size= 128\n\n#running_model\nhistory = bilstm_model.fit(\n    X_train, \n    y_train, \n    epochs = epoch_count,\n    batch_size = batch_size,\n    validation_data = (x_val, y_val),\n    verbose = 1,\n    callbacks = [reduce_lr],\n    class_weight=class_weight\n)\n\n#plotting graphs\ndef plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=17)\n        ax[idx].set_xlabel('Loss ',fontsize=14)\n        ax[idx].set_ylabel('Accuracy',fontsize=14)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n\nplot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n\n#prediciting\npreds= np.argmax(bilstm_model.predict(X_test), axis=-1)\n#printing classification_report & confusion_matrix\nprint(classification_report(y_test,preds ))\nprint(confusion_matrix(y_test, preds))","metadata":{"execution":{"iopub.status.busy":"2022-08-14T10:59:20.778484Z","iopub.execute_input":"2022-08-14T10:59:20.778863Z","iopub.status.idle":"2022-08-14T11:49:16.626713Z","shell.execute_reply.started":"2022-08-14T10:59:20.778824Z","shell.execute_reply":"2022-08-14T11:49:16.625806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model","metadata":{}},{"cell_type":"code","source":"filters= 32\nkernel_size=2\nhidden_dims= 128\n    \ndef CNN():\n    model = Sequential()\n    #adding embedding layer\n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence))\n    # 2 CNN layer\n    model.add(Conv1D(32,2,padding='valid', activation='relu')) #cnn_layer_1\n    model.add(Conv1D(64,2,padding='valid',activation='relu')) #cnn_layer_2\n    model.add(GlobalMaxPooling1D()) #globalmaxpooling_layer\n    model.add(Dense(256, activation='relu')) #dense_layer\n    model.add(Dropout(0.1)) #dropout_layer\n    model.add(Dense(3, activation = 'softmax')) #classification layer\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model\n\n#builiding CNN model\nmodel2=CNN()\n \n#running mode\nhistory2 = model2.fit(\n    X_train, \n    y_train, \n    epochs = epoch_count,\n    batch_size = batch_size,\n    validation_data = (x_val, y_val),\n    verbose = 1,\n    callbacks = [reduce_lr],\n    class_weight=class_weight\n)\n\n#plotting graphs\nplot_learning_curves(history2, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n#predicting\npred2= np.argmax(model2.predict(X_test), axis=-1)\n#printing reports\nprint(classification_report(y_test,pred2 ))\nprint(confusion_matrix(y_test, pred2))","metadata":{"execution":{"iopub.status.busy":"2022-08-14T11:49:16.629015Z","iopub.execute_input":"2022-08-14T11:49:16.629642Z","iopub.status.idle":"2022-08-14T11:49:38.352960Z","shell.execute_reply.started":"2022-08-14T11:49:16.629583Z","shell.execute_reply":"2022-08-14T11:49:38.351900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MLP Model","metadata":{}},{"cell_type":"code","source":"def MLP():\n    model = Sequential()\n    #embedding layer\n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence))\n    model.add(Flatten()) #flatten_layer\n    model.add(Dense(512, activation='relu')) #dense_layer\n    model.add(Dropout(0.2)) #dropout_layer\n    model.add(Dense(3, activation = 'softmax'))#classification_layer\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model\n\n#building model\nmodel3=MLP()\n#running_model\nhistory3 = model3.fit(\n    X_train, \n    y_train, \n    epochs = epoch_count,\n    batch_size = batch_size,\n    validation_data = (x_val, y_val),\n    verbose = 1,\n    callbacks = [reduce_lr],\n    class_weight=class_weight\n)\n\n#plotting_graphs\nplot_learning_curves(history3, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n#predicting\npred3= np.argmax(model3.predict(X_test), axis=-1)\n\nprint(classification_report(y_test,pred3))\nprint(confusion_matrix(y_test, pred3))","metadata":{"execution":{"iopub.status.busy":"2022-08-14T11:49:38.354505Z","iopub.execute_input":"2022-08-14T11:49:38.355117Z","iopub.status.idle":"2022-08-14T11:49:59.960508Z","shell.execute_reply.started":"2022-08-14T11:49:38.355080Z","shell.execute_reply":"2022-08-14T11:49:59.959410Z"},"trusted":true},"execution_count":null,"outputs":[]}]}